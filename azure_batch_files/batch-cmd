#!/usr/bin/env bash

# Command script to run distSim application on multiple machines as MPI task on Azure Batch.

NUM_NODES=$1
PROCESS_PER_NODE=$2
AZ_BATCH_TASK_SHARED_DIR=$3
AZ_BATCH_HOST_LIST=$4
RESULTS_NAME=$5
COMM_PROTOCOL=$6
DISTRIBUTION=$7
GLOBAL_SEED=$8


# total number of processes is number of nodes multipled by number of processes per node
(( numProc = $NUM_NODES * $PROCESS_PER_NODE ))

# environment variables
#export PATH="/home/openmpi-3.1.1/build/bin:$PATH"
#export LD_LIBRARY_PATH="/home/openmpi-3.1.1/build/lib/:$LD_LIBRARY_PATH"
export PATH="$PATH:/home/qt_build/bin"
export PATH="$PATH:/home/scorep-3.0/build/bin"
export PATH="$PATH:/home/cube-4.3.4/build/bin"
export PATH="$PATH:/home/scalasca2_build/bin"

# if using Intel MPI + RDMA
#source /opt/intel/impi/2017.3.196/bin64/mpivars.sh
source /opt/intel/impi/5.1.3.223/bin64/mpivars.sh
export I_MPI_SHM_LMT=shm
# can use tcp (if only one fabric stated, it is used for intra and inter node comm)
export I_MPI_FABRICS=shm:tcp	# RDMA (dapl), normal (tcp), shared mem (shm)
# THIS IS A MANDATORY ENVIRONMENT VARIABLE AND MUST BE SET BEFORE RUNNING ANY JOB
# Setting the variable to shm:dapl gives best performance for some applications
# If your application doesnâ€™t take advantage of shared memory and MPI together, then set only dapl
export I_MPI_DAPL_PROVIDER=ofa-v2-ib0
# THIS IS A MANDATORY ENVIRONMENT VARIABLE AND MUST BE SET BEFORE RUNNING ANY JOB
export I_MPI_DYNAMIC_CONNECTION=0
export I_MPI_FALLBACK=1
export I_MPI_DEBUG=0
export I_MPI_JOB_RESPECT_PROCESS_PLACEMENT=0
export I_MPI_PERHOST=$PROCESS_PER_NODE
###############

hostList=$(echo $AZ_BATCH_HOST_LIST | tr "," "\n")

HOST_LIST=""
for addr in $hostList
do
    HOST_LIST=$HOST_LIST$addr":"$PROCESS_PER_NODE","
done



# BATCHED EXPERIMENT PARAMETERS
ITERATIONS=1 # how many different seeds used
REPETITIONS=1 # how many times each individual experiment (single seed) is repeated
H_FILE="2D_54019_highK.mtx.hgr"

# 0. needed to fill pipeline (otherwise first run suffers from low MPI communication speed)
#intel MPI
echo "Warming pipeline"
mpirun -n $numProc -ppn $PROCESS_PER_NODE --host $HOST_LIST -wdir $AZ_BATCH_TASK_SHARED_DIR $AZ_BATCH_TASK_SHARED_DIR/hello_world

sleep 10

# 1. PROFILE NETWORK ARCHITECTURE WITH MPI_PERF
mpirun -n $numProc -ppn $PROCESS_PER_NODE --host $HOST_LIST -wdir $AZ_BATCH_TASK_SHARED_DIR $AZ_BATCH_TASK_SHARED_DIR/mpi_perf 16000 100 50 network_profile

network_profile_root="network_profile_mpi"
network_profile_file=$network_profile_root"_send_bandwidth_"$numProc

sleep 3

# 2. RUN NULL SIMULATIONS

run_partitioning_experiment() {
	echo "Another experiment"
	MODEL_NAME="$1"
	COMM_PATTERN="$2"
	PARTITIONING="$3"
	SEED="$4"
	HYPERGRAPH_FILE=$AZ_BATCH_TASK_SHARED_DIR"/"$5
	if [ $6 == "yes" ]
	then
		RESULTS=$MODEL_NAME"_withbandwidth"
	else
		RESULTS=$MODEL_NAME
	fi
	
	if [ $6 == "yes" ]
	then
		BANDWIDTH_FILE="-b "$AZ_BATCH_TASK_SHARED_DIR"/"$network_profile_file
	else
		BANDWIDTH_FILE=""
	fi
	
	for i in $(seq 1 $REPETITIONS)
	do
		mpirun -n $numProc -ppn $PROCESS_PER_NODE --host $HOST_LIST -wdir $AZ_BATCH_TASK_SHARED_DIR $AZ_BATCH_TASK_SHARED_DIR/distSim -n $RESULTS -c $COMM_PATTERN -p $PARTITIONING -s $SEED -k 1000 -f 1000 -t 1000 -i $PROCESS_PER_NODE -N -h $HYPERGRAPH_FILE $BANDWIDTH_FILE
		sleep 1
	done
}



for i in $(seq 1 $ITERATIONS)
do
	echo "Iteration $i"
	RND=$RANDOM
	run_partitioning_experiment $RESULTS_NAME "nbx" "prawP" $RND $H_FILE "no"
	run_partitioning_experiment $RESULTS_NAME "nbx" "prawS" $RND $H_FILE "no"
	run_partitioning_experiment $RESULTS_NAME "nbx" "prawP" $RND $H_FILE "yes"
	run_partitioning_experiment $RESULTS_NAME "nbx" "prawS" $RND $H_FILE "yes"
	run_partitioning_experiment $RESULTS_NAME "nbx" "zoltanFile" $RND $H_FILE "no"
done



# prepare the output and compress it as result
mkdir results
cp $AZ_BATCH_TASK_SHARED_DIR/$RESULTS_NAME* results/ # simulation results
cp $AZ_BATCH_TASK_SHARED_DIR/$network_profile_root* results/ # network profile results
cp $AZ_BATCH_TASK_SHARED_DIR/$H_FILE* results/ # hypergraph partitioning stats
rm -f "$AZ_BATCH_TASK_SHARED_DIR/RESULTS.tgz"
cd results
tar -czf "$AZ_BATCH_TASK_SHARED_DIR/RESULTS.tgz" *
exit

